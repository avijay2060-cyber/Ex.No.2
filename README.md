
# Ex.No: 2 	Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta 
### NAME : A.VIJAY
### DATE: 29.08.2025                                                                           
### REGISTER NUMBER : 212223080055

 
### Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a Prompt based output using different Prompting tools of 2024.
### AI Tools required:

### Explanation:
Define the Use Case:
Select a specific task for evaluation across platforms (e.g., summarizing a document, answering a technical question, or generating a creative story / Code).
Ensure the use case is applicable to all platforms and will allow for comparison across response quality, accuracy, and depth.
Create a Set of Prompts:
Prepare a uniform set of prompts that align with the chosen use case.
Each prompt should be clear and precise, ensuring that all platforms are evaluated using the same input.
Consider multiple prompts to capture the versatility of each platform in handling different aspects of the use case.
Run the Experiment on Each AI Platform:
Input the prompts into each AI tool (ChatGPT, Claude, Bard, Cohere Command, and Meta) and gather the responses.
Ensure the same conditions are applied for each platform, such as input format, time to respond, and prompt delivery.
Record response times, ease of interaction with the platform, and any technical issues encountered.
Evaluate Response Quality:
Assess each platform‚Äôs responses using the following criteria: Accuracy,Clarity,Depth,Relevance 
Compare Performance:
Compare the collected data to identify differences in performance across platforms.
Identify any platform-specific advantages, such as faster response times, more accurate answers, or more intuitive interfaces.
Deliverables:
A comparison table outlining the performance of each platform (ChatGPT, Claude, Bard, Cohere Command, and Meta) based on accuracy, clarity, depth, and relevance of responses.
A final report summarizing the findings of the experiment, including recommendations on the most suitable AI platform for different use cases based on performance and user 

### Output:
## üîπ 1. Zero-Shot Prompting

**Prompt:**
*"Summarize the following technical abstract in simple terms."*

* **ChatGPT (OpenAI):** Clear, concise summary with simplified language.
* **Claude (Anthropic):** More narrative, tends to expand with extra context.
* **Bard (Google):** Concise but sometimes includes speculative extensions.
* **Cohere Command:** Direct, factual, but less nuanced explanation.
* **Meta LLaMA:** Technical, closer to original wording, less simplified.

---

## üîπ 2. Few-Shot Prompting

**Prompt:**
\*"Example:
Input: 'This paper presents an AI model for disease prediction.'
Output: 'The paper introduces an AI system that helps doctors predict diseases early.'

Now summarize this text: \[Technical Abstract]"\*

* **ChatGPT:** Matches the training style, adapts tone well.
* **Claude:** Provides slightly longer summary but coherent.
* **Bard:** Tries to mimic example exactly, sometimes rigid.
* **Cohere:** Concise, accurate, but lacks stylistic variety.
* **Meta LLaMA:** Keeps academic tone, not very ‚Äúsimple.‚Äù

---

## üîπ 3. Chain-of-Thought Prompting (Reasoning First)

**Prompt:**
*"Step 1: Identify the goal of the research.
Step 2: Identify the method used.
Step 3: Summarize in plain English."*

* **ChatGPT:** Follows structured steps clearly.
* **Claude:** Very detailed in reasoning, explains method + impact.
* **Bard:** Sometimes skips steps, jumps to final summary.
* **Cohere:** Step breakdown less detailed, final summary okay.
* **Meta LLaMA:** Keeps steps technical, less user-friendly.

---

## üîπ 4. Role Prompting

**Prompt:**
*"You are a university professor explaining this research to undergraduate students. Summarize in simple, engaging language."*

* **ChatGPT:** Friendly, student-focused explanation.
* **Claude:** Conversational, adds analogies.
* **Bard:** Simplified, but less engaging.
* **Cohere:** Direct, plain, lacks metaphorical support.
* **Meta LLaMA:** Still technical, less adapted to role.

---

## üîπ 5. Tabular Format Prompting

**Prompt:**
*"Summarize the abstract in a table with columns: Goal | Method | Results | Applications."*

| Platform       | Goal                            | Method                   | Results                                  | Applications                        |
| -------------- | ------------------------------- | ------------------------ | ---------------------------------------- | ----------------------------------- |
| ChatGPT        | Low-power, area-efficient adder | MGDI-based CSLA design   | Reduced transistor count, improved delay | VLSI design, energy-efficient chips |
| Claude         | Same as ChatGPT, more verbose   | Adds detail on CMOS tech | Energy efficiency, scalability           | VLSI, scalable processors           |
| Bard           | Same, but less detail           | Focus on CSLA variation  | Mentions speed & power improvements      | General electronics, processors     |
| Cohere Command | Minimal, factual                | MGDI                     | Power & delay improvements               | VLSI                                |
| Meta LLaMA     | Technical phrasing              | CMOS + MGDI              | Simulation improvements                  | Hardware architecture research      |

---

## üîπ Key Insights (2024 Prompting Tools)

* **ChatGPT**: Balanced, adaptable, strong role/tabular formatting.
* **Claude**: Rich reasoning, strong for step-by-step or analogies.
* **Bard**: Fast + concise, but sometimes speculative.
* **Cohere Command**: Direct, factual, less nuanced.
* **Meta LLaMA**: Technical, closer to research style, weaker on simplification.

---

### Conclusion: 


# Result : The Prompt for the above problem complete successfully.
